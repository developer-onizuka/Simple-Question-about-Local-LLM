# 1. ローカルLLMとは何か？
ローカルLLMは、Large Language Model (LLM) をユーザーのPCやデバイス上で直接実行する方法論。通常、LLMは膨大な計算能力を持つクラウドサーバー上で動作するが、ローカルLLMはより効率的な設計や量子化技術によって、個人が所有するコンピュータや企業のオンプレミスサーバーでも動かせるように最適化されているが、以下に示す課題がある。

| 項目 | 説明 |
| :--- | :--- |
| **性能の制約** | クラウド上の大規模なLLMに比べ、モデルサイズが小さいため、生成されるテキストの品質や複雑なタスクの処理能力が劣る場合がある。 |
| **ハードウェアの要求** | 動作に必要なメモリやGPU性能は日々低くなっているが、高性能なモデルを快適に動かすには、依然として高性能なGPUが不可欠。 |
| **セットアップの複雑さ** | モデルのダウンロード、ソフトウェアのインストール、設定など、技術的な知識がある程度必要になる場合がある。 |

# 2. これらの課題の代表的な解決策は？
- 量子化 (Quantization)<br>
モデルのサイズを劇的に小さくし、計算に必要なリソースを削減するための最も重要な技術である。LLMは通常、32ビットの浮動小数点数（FP32）でパラメーターを格納するが、量子化はこれを8ビット（INT8）や4ビット（INT4）といった、より小さなデータ型の整数に変換する。これにより、モデルのファイルサイズとメモリ使用量が大幅に減少し、少ない計算能力でも推論を実行できるようになる。<br>
- 推論エンジン (Inference Engine)<br>
量子化されたモデルを効率的に実行するためのフレームワークである。これらは、GPUやCPUのアーキテクチャに最適化されており、推論を高速化する。代表的なものには、GGML、GGUF、llama.cpp、Ollama、vLLM などがある。これらの推論エンジンは、CPUだけでもモデルを実行できるように設計されていることが多く、高性能なGPUを持たないユーザーでもローカルLLMを実行可能となる。

# 3. 推論エンジンで扱われるGGUFとは何か？
GGUF (GPT-Generated Unified Format)は、ローカルLLMの実行を最適化するために設計されたファイル形式である。これは、LLMを効率的に保存し、さまざまなデバイス上で高速に推論できるようになることを目的としている。GGUFは、llama.cppという人気の高い推論エンジンの開発チームによって、その前身であるGGMLの後継として開発された。
- GGUF形式の登場により、ローカル環境でLLMを動かすハードルが大きく下がった。主な特徴としては以下があげられる。<br>

| 特徴 | 説明 |
| :--- | :--- |
| **単一ファイル** | モデルの重み、トークナイザー、その他のメタデータがすべて**1つのファイル**にまとめられ、モデルの管理や共有が簡単になる。 |
| **高速なロード** | メモリマップドファイル（mmap）を使用することで、モデルをディスクから**高速にロード**可能。これにより、モデルの起動時間を大幅に短縮できる。 |
| **マルチプラットフォーム対応** | さまざまなOSやハードウェア（CPU、GPUなど）で動作するように設計されており、**幅広い環境**での利用が可能。特に、**CPUでの推論**に最適化される。 |

- なお、Ollamaやllama.cppは、GGUF形式のモデルを直接インポートして実行することができ、Hugging Faceなどで公開されているGGUFファイルをダウンロードし、クライアントPCで簡単にLLMを扱うことが可能。
- Ollamaのコミュニティでは、公開されている多くの人気モデルがGGUF形式で提供されており、ユーザーは手軽にローカル環境でモデルを試すことが可能である。

# 4. 各推論エンジンの違いを知りたい。特に、Llama.cppとOllamaは同じように見えるが理解は正しいか？
LLM推論エンジンの比較を以下に示す。Llama.cppとOllamaは同じように見えるという理解は概ね正しい。

| | **Llama.cpp** | **Ollama** | **vLLM** |
| :--- | :--- | :--- | :--- |
| **役割** | LLMの推論エンジン（ライブラリ）。 | LLM実行のプラットフォーム（アプリケーション）。 | LLMの高速推論エンジン。 |
| **主な対象ユーザー** | コア技術を扱いたい開発者、研究者。 | 手軽にローカルLLMを使いたい人、API連携したい開発者。 | 高スループットのLLMサービングを求める開発者、研究者。 |
| **主要なモデル形式** | GGUF | GGUF | PyTorch, Safetensors (Hugging Face互換)。 |
| **主な実行環境** | CPUを主とし、GPUもサポート。 | CPU、GPU (NVIDIA, AMDなど)。 | GPU (NVIDIAを主とし、ROCmでAMDもサポート)。 |
| **最適化アプローチ** | **事前コンパイル**によるシンプルさと移植性。 | Llama.cppをバックエンドとして利用し、使いやすさを提供。 | **JITコンパイル**と独自のアルゴリズム（PagedAttention）による高スループット。 |
| **OpenAI互換** | 限定的 | 高い互換性（`ollama serve`） | 高い互換性（`api_server`） |


# 5. vLLMでもGGUFを使うのか？
- vLLMはGGUF形式のモデルをネイティブにはサポートしておらず、Hugging Faceのtransformersライブラリと密接に連携しており、Hugging Face Hubで公開されているPyTorchやSafetensors形式のモデルを主要なサポート対象としているものである。
- これらのモデルは、vLLMの高速な推論エンジン（特に、Continuous BatchingやPagedAttentionといった技術）を活用するために、特定の形式でロードされることとなる。

# 6. なぜvLLMはGGUFをサポートしないのか？
- vLLMとLlama.cpp/Ollamaは、それぞれ異なる目的と最適化の方向性を持っており、vLLMの目的が、大規模なオンライン推論サーバーでの高スループット、低レイテンシを実現するためである。特にNVIDIA GPUに強く最適化されている。
- これに対し、Llama.cpp/Ollamaは、CPUやコンシューマ向けGPUといった、より身近なハードウェアでLLMを動かすといったことが目的となる。主に、CPUを対象とし、GPUもサポート（CUDA, Metal, ROCmなど）している。
- これらの違いから、vLLMがGGUF形式を直接サポートする必要性は現時点では低いと見なされ、GGUFはCPUでの効率的な推論に強みを持つ一方、vLLMはGPU上での高スループット推論に特化しているため、それぞれの得意分野が異なると言える。

# 7. vLLMが扱うモデルの形式を知りたい。
- vLLMが主にサポートしているのは、Hugging Face Transformersライブラリと互換性のあるモデル形式で、具体的には、以下の形式を扱うことになる。

| 形式 | 拡張子 | 特徴 |
| :--- | :--- | :--- |
| **PyTorch形式** | .bin または .pt | モデルの重みをPyTorchのテンソルとして保存。一般的な形式。 |
| **Safetensors形式** | .safetensors | PyTorchモデルをより安全かつ高速にロードするために設計された形式。最近のモデルで多く採用されている。 |

- vLLMは、これらの形式のモデルをHugging Face Hubから直接ダウンロードして利用することを前提としている。vllm.LLMクラスにモデル名（例: "meta-llama/Llama-3-8B-Instruct"）を指定するだけで、自動的にダウンロードしてロードしてくれる。

# 8. vLLMはOpenAI互換か？
- vLLMはOpenAI APIと互換性のあるサーバー機能を提供している。vLLMをサーバーモードで起動すると、そのエンドポイントはOpenAIのChat Completions APIやCompletions APIと同じインターフェースを持つようになり、Open WebUIなどのエコシステムを用いることができる。
```
pip install vllm
python -m vllm.entrypoints.openai.api_server --model "meta-llama/Llama-3-8B-Instruct"
```
- このコマンドで、指定したモデルがロードされ、OpenAI APIと互換性のあるHTTPサーバーが起動し、開発者は、OpenAIのAPIキーやベースURLを、vLLMサーバーのエンドポイントに置き換えるだけで、既存のアプリケーションをそのまま利用可能。
- このように、vLLMは単なる推論ライブラリではなく、実用的なAPIサーバーとしても利用でき、OpenAI APIに依存するアプリケーションをローカルで実行す手段を提供するものである。

# 9. vLLMはAMDのGPUでも動くか?
- vLLMはAMDのGPUでも動作可能であるが、NVIDIAのGPUと比較すると、まだサポートは限定的。

# 10. なぜ同じバイナリが異なるアーキテクチャのGPUで動作できるのか？
- GPUはメーカー（NVIDIA、AMD、Intelなど）ごとに、そして世代ごとに異なる独自のアーキテクチャ（命令セットやメモリ構造など）を持っており、そのため、原則として、ある特定のGPU用にコンパイルされたバイナリは、他のアーキテクチャのGPUでは直接動作しない。しかし、vLLMのようなライブラリが異なるメーカーのGPUでも動作するのは、メーカーが提供する抽象化レイヤーが大きく関係している。
- 例えば、vLLMは内部でPyTorchを使うが、PyTorchはNVIDIAのGPUを検出するとCUDAのAPIを、AMDのGPUを検出するとROCmのAPIを自動的に利用するように設計されているため、vLLMは、transformersモデルのPyTorch/Safetensors形式の重みと、推論に必要なオペレーション（PagedAttentionなど）を、この抽象化レイヤーを介してGPUに送る。
- この結果、ユーザーは、GPUがNVIDIAであれAMDであれ、vLLMの同じ高レベルのコード（LLMクラス）を使うだけで、内部で適切なGPUコードが実行されることになる。
- これを「JIT (Just-in-Time) コンパイル」呼び、実行の直前にモデルがコンパイルされることになる。（正確には、「実行の直前にコンパイルされる部分」と「事前にコンパイルされた部分」の2つが組み合わさっている。）
- 多くのGPU向けディープラーニングフレームワーク（PyTorch、TensorFlowなど）や、vLLMのようなライブラリは、このJITコンパイルを積極的に利用しており、モデルのサイズや推論のパラメータ（例えば、バッチサイズ）が変わるたびに、最適な計算グラフやカーネルをその場で生成・コンパイルすることで、パフォーマンスを最大化する。
- しかし、JITコンパイルのトレードオフとして、初めて特定の計算を行う際、コンパイルのオーバーヘッドが発生するため、起動時や初回推論に時間がかかることがある。このオーバーヘッドを減らすため、一度コンパイルされたバイナリはディスク上にキャッシュされることが一般的。そのため、2回目以降の実行では、コンパイルの手間が省かれ、高速な動作が可能。


